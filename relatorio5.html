<div class="relatorio-container">
  <h2>Relat√≥rio 5 - Extra√ß√£o de Caracter√≠sticas - Features</h2>
  <h3><pre>
  Leonardo Severgnine Maioli - RA: 11201920579
  Ricardo Javurek Rihan - RA: 11201920897
  Tiago Luiz Silva de Araujo Pereira - RA: 11013316

  Laborat√≥rio finalizado dia 28 de Julho
  </pre></h3>

  <h2>Introdu√ß√£o</h2>
  <p>
    A extra√ß√£o de caracter√≠sticas √© uma etapa essencial na Vis√£o Computacional, respons√°vel por identificar pontos, regi√µes ou padr√µes relevantes em uma imagem, que podem ser utilizados para diversas tarefas como reconhecimento de objetos, rastreamento, reconstru√ß√£o 3D, entre outras. Esses pontos s√£o conhecidos como "features", e s√£o projetados para serem √∫nicos e consistentes mesmo sob mudan√ßas de escala, rota√ß√£o ou ilumina√ß√£o.
  </p>
  <p>
    Neste laborat√≥rio, exploramos diferentes abordagens de detec√ß√£o e descri√ß√£o de caracter√≠sticas, com destaque para os detectores Harris e Shi-Tomasi, al√©m do m√©todo SIFT (Scale-Invariant Feature Transform), que permite identificar e descrever pontos-chave de forma robusta. Tamb√©m foi abordada a Transformada de Hough, uma t√©cnica cl√°ssica para detec√ß√£o de formas geom√©tricas como linhas e c√≠rculos.
  </p>
  <p>
    O objetivo das atividades propostas foi tanto o entendimento te√≥rico desses m√©todos quanto sua aplica√ß√£o pr√°tica utilizando a biblioteca OpenCV em Python. A detec√ß√£o de correspond√™ncias entre imagens e a an√°lise de v√≠deos ao vivo a partir de uma c√¢mera est√©reo previamente calibrada foram algumas das aplica√ß√µes desenvolvidas para consolidar os conceitos.
  </p>

  <h2>Arquivos base para reproduzir o procedimento:</h2>
  <ul>
    <li><a href="relatorio5\esza019_2025.2__Lab5_features_.pdf" download class="download-link">üìÑ Baixar Roteiro (PDF)</a>
  </ul>

  <h2>Procedimentos Experimentais</h2>

  <h3>Parte 1. Estudo da teoria</h3>
  <p>
    Antes da implementa√ß√£o dos experimentos, foram estudados os fundamentos te√≥ricos por meio de tutoriais e materiais oficiais da documenta√ß√£o OpenCV. Inicialmente, foi abordado o conceito de "features", entendendo sua import√¢ncia na an√°lise de imagens e sua aplica√ß√£o em sistemas de vis√£o. Em seguida, foram estudados diferentes detectores de pontos de interesse:
  </p>
  <ul>
    <li><a href="https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html" target="_blank">Entendendo sobre Features</a>: Introdu√ß√£o aos conceitos de caracter√≠sticas em imagens e seu papel em tarefas como reconhecimento e rastreamento.</li>
    <li><a href="https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html" target="_blank">Detector de Harris</a>: T√©cnica cl√°ssica para detec√ß√£o de cantos, baseada em mudan√ßas de intensidade em diferentes dire√ß√µes.</li>
    <li><a href="https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html" target="_blank">Detector de Shi-Tomasi</a>: Varia√ß√£o do detector de Harris que melhora a sele√ß√£o de cantos mais est√°veis.</li>
    <li><a href="https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html" target="_blank">Introdu√ß√£o ao SIFT</a>: Algoritmo robusto para detec√ß√£o e descri√ß√£o de pontos-chave invariante √† escala e rota√ß√£o.</li>
    <li><a href="https://learnopencv.com/hough-transform-with-opencv-c-python/" target="_blank">Transformada de Hough</a>: T√©cnica usada para detec√ß√£o de formas geom√©tricas, como linhas e c√≠rculos, em imagens com ru√≠do ou objetos parciais.</li>
  </ul>
  <p>
    Esses estudos forneceram a base conceitual necess√°ria para o desenvolvimento dos experimentos, possibilitando uma compreens√£o das ferramentas aplicadas nas etapas seguintes.
  </p>

  <h3>Parte 2A. Feature Matching com Homografia</h3>
  <p>
    Nessa primeira etapa pr√°tica, desenvolvemos um script baseado no tutorial da OpenCV disponibilizado em: 
    <a href="https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html" target="_blank">Feature Matching + Homography</a>. 
    O c√≥digo realiza a detec√ß√£o de caracter√≠sticas com SIFT em duas imagens distintas de um mesmo objeto, localizadas em posi√ß√µes e cenas diferentes. Utilizamos correspond√™ncia de features e estimativa de homografia para identificar a localiza√ß√£o do objeto na cena. O resultado final √© uma imagem com as correspond√™ncias visuais tra√ßadas.
  </p>

  <div class="image-row">
    <div class="image-col">
      <img src="relatorio5/box.png" alt="Imagem do objeto isolado (box.png)" class="relatorio-image-small">
      <p class="image-caption"><em>Imagem 1: Objeto isolado (box.png)</em></p>
    </div>
    <div class="image-col">
      <img src="relatorio5/box_in_scene.png" alt="Objeto em cena (box_in_scene.png)" class="relatorio-image-small">
      <p class="image-caption"><em>Imagem 2: Objeto em outra cena (box_in_scene.png)</em></p>
    </div>
    <div class="image-col">
      <img src="relatorio5/featureMatchting.png" alt="Imagem com correspond√™ncias tra√ßadas" class="relatorio-image">
      <p class="image-caption"><em>Imagem 3: Correspond√™ncia de features detectadas (featureMatching.png)</em></p>
    </div>
  </div>

  <div class="button-group">
    <button onclick="toggleSection('code-parte2a')">Ver C√≥digo - Parte 2A</button>
  </div>
  <div id="code-parte2a" class="relatorio-section" style="display: none;">
    <code><pre>
  import numpy as np
  import cv2 as cv
  from matplotlib import pyplot as plt

  MIN_MATCH_COUNT = 10

  img1 = cv.imread('box.png', cv.IMREAD_GRAYSCALE)          # queryImage
  img2 = cv.imread('box_in_scene.png', cv.IMREAD_GRAYSCALE) # trainImage

  # Initiate SIFT detector
  sift = cv.SIFT_create()

  # find the keypoints and descriptors with SIFT
  kp1, des1 = sift.detectAndCompute(img1, None)
  kp2, des2 = sift.detectAndCompute(img2, None)

  FLANN_INDEX_KDTREE = 1
  index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
  search_params = dict(checks=50)

  flann = cv.FlannBasedMatcher(index_params, search_params)
  matches = flann.knnMatch(des1, des2, k=2)

  # store all the good matches as per Lowe's ratio test
  good = []
  for m, n in matches:
      if m.distance < 0.7 * n.distance:
          good.append(m)

  if len(good) > MIN_MATCH_COUNT:
      src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
      dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

      M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)
      matchesMask = mask.ravel().tolist()

      h, w = img1.shape
      pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
      dst = cv.perspectiveTransform(pts, M)

      img2 = cv.polylines(img2, [np.int32(dst)], True, 255, 3, cv.LINE_AA)

  else:
      print("Not enough matches are found - {}/{}".format(len(good), MIN_MATCH_COUNT))
      matchesMask = None

  draw_params = dict(matchColor=(0, 255, 0),
                    singlePointColor=None,
                    matchesMask=matchesMask,
                    flags=2)

  img3 = cv.drawMatches(img1, kp1, img2, kp2, good, None, **draw_params)
  plt.imshow(img3, 'gray'), plt.show()
    </pre></code>
  </div>

  <h3>Parte 2B. Matching ao vivo com webcams</h3>
  <p>
    Nessa parte B, adaptamos o c√≥digo anterior para realizar a detec√ß√£o de caracter√≠sticas ao vivo com as duas c√¢meras da configura√ß√£o est√©reo. O sistema capturava continuamente os quadros de ambas as webcams e realizava a correspond√™ncia de features em tempo real, destacando visualmente os pontos correspondentes.
  </p>
  <p>
    Abaixo apresentamos uma imagem de exemplo obtida durante a execu√ß√£o do sistema, demonstrando o funcionamento da correspond√™ncia de caracter√≠sticas entre os dois v√≠deos:
  </p>

  <div class="image-row">
    <div class="image-col">
      <img src="relatorio5/featureMatchtingVideo.png" alt="Correspond√™ncia em v√≠deo ao vivo com c√¢meras est√©reo" class="relatorio-image">
      <p class="image-caption"><em>Exemplo de correspond√™ncia de features entre as duas c√¢meras (v√≠deo ao vivo)</em></p>
    </div>
  </div>

  <div class="button-group">
    <button onclick="toggleSection('code-parte2b')">Ver C√≥digo - Parte 2B</button>
  </div>
  <div id="code-parte2b" class="relatorio-section" style="display: none;">
    <code><pre>
  import cv2 as cv
  import numpy as np

  MIN_MATCH_COUNT = 10

  cams = []
  for i in range(10):
      camera = cv.VideoCapture(i)
      if camera.isOpened():
          cams.append(i)
      camera.release()

  # Inicia captura das duas c√¢meras (ajuste os √≠ndices conforme necess√°rio)
  cap_left = cv.VideoCapture(cams[0])
  cap_right = cv.VideoCapture(cams[1])

  if not cap_left.isOpened() or not cap_right.isOpened():
      print("Erro ao abrir as c√¢meras")
      exit()

  sift = cv.SIFT_create()

  FLANN_INDEX_KDTREE = 1
  index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
  search_params = dict(checks=50)
  flann = cv.FlannBasedMatcher(index_params, search_params)

  while True:
      ret1, frame_left = cap_left.read()
      ret2, frame_right = cap_right.read()

      if not ret1 or not ret2:
          print("Erro ao capturar quadros")
          break

      gray_left = cv.cvtColor(frame_left, cv.COLOR_BGR2GRAY)
      gray_right = cv.cvtColor(frame_right, cv.COLOR_BGR2GRAY)

      kp1, des1 = sift.detectAndCompute(gray_left, None)
      kp2, des2 = sift.detectAndCompute(gray_right, None)

      if des1 is not None and des2 is not None:
          matches = flann.knnMatch(des1, des2, k=2)

          good = []
          for m, n in matches:
              if m.distance < 0.7 * n.distance:
                  good.append(m)

          matchesMask = None
          if len(good) > MIN_MATCH_COUNT:
              src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
              dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

              M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)
              matchesMask = mask.ravel().tolist()

              h, w = gray_left.shape
              pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)
              dst = cv.perspectiveTransform(pts, M)

              frame_right = cv.polylines(frame_right, [np.int32(dst)], True, (255, 0, 0), 3, cv.LINE_AA)
          else:
              print("Matches insuficientes - {}/{}".format(len(good), MIN_MATCH_COUNT))

          draw_params = dict(matchColor=(0, 255, 0),
                            singlePointColor=None,
                            matchesMask=matchesMask,
                            flags=2)

          img_matches = cv.drawMatches(frame_left, kp1, frame_right, kp2, good, None, **draw_params)
          cv.imshow("Matches", img_matches)

      if cv.waitKey(1) & 0xFF == ord('q'):
          break

  cap_left.release()
  cap_right.release()
  cv.destroyAllWindows()
    </pre></code>
  </div>

  <h3>Parte 3C. Transformada de Hough em imagens</h3>
  <p>
    Nessa pr√≥xima etapa do laborat√≥rio, implementamos um script baseado no tutorial disponibilizado em: 
    <a href="https://learnopencv.com/hough-transform-with-opencv-c-python/" target="_blank">Hough Transform</a>, 
    que aplica a Transformada de Hough para detectar linhas e c√≠rculos em imagens previamente capturadas. Essa t√©cnica foi essencial para identifica√ß√£o de formas geom√©tricas em cenas controladas.
  </p>

  <div class="image-row">
    <div class="image-col">
      <img src="relatorio5/circles.jpg" alt="Imagem original para detec√ß√£o de c√≠rculos" class="relatorio-image">
      <p class="image-caption"><em>Imagem 1: Original (para detec√ß√£o de c√≠rculos)</em></p>
    </div>
    <div class="image-col">
      <img src="relatorio5/HoughCircle.png" alt="Resultado da Transformada de Hough para c√≠rculos" class="relatorio-image">
      <p class="image-caption"><em>Imagem 2: C√≠rculos detectados (HoughCircle.png)</em></p>
    </div>
  </div>

  <div class="image-row">
    <div class="image-col">
      <img src="relatorio5/images.jpg" alt="Imagem original para detec√ß√£o de linhas" class="relatorio-image">
      <p class="image-caption"><em>Imagem 3: Original (para detec√ß√£o de linhas)</em></p>
    </div>
    <div class="image-col">
      <img src="relatorio5/HoughLines.png" alt="Resultado da Transformada de Hough para linhas" class="relatorio-image">
      <p class="image-caption"><em>Imagem 4: Linhas detectadas (HoughLines.png)</em></p>
    </div>
  </div>

  <div class="button-group">
    <button onclick="toggleSection('code-parte3c')">Ver C√≥digo - Parte 3C</button>
  </div>
  <div id="code-parte3c" class="relatorio-section" style="display: none;">
    <code><pre>
  import cv2
  import numpy as np
  import glob

  # Caminho para as imagens
  imagens = glob.glob("*.jpg")  # ajuste o caminho conforme necess√°rio

  for img_path in imagens:
      img = cv2.imread(img_path)
      gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
      blurred = cv2.medianBlur(gray, 5)

      # Detec√ß√£o de bordas
      edges = cv2.Canny(blurred, 50, 150)

      # Detectar linhas com HoughLinesP
      linhas = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=50, maxLineGap=10)
      if linhas is not None:
          for linha in linhas:
              x1, y1, x2, y2 = linha[0]
              cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)

      # Detectar c√≠rculos com HoughCircles
      circulos = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT, 1, 50,
                                  param1=50, param2=60, minRadius=5, maxRadius=100)
      if circulos is not None:
          circulos = np.uint16(np.around(circulos))
          for c in circulos[0, :]:
              cv2.circle(img, (c[0], c[1]), c[2], (255, 0, 0), 2)
              cv2.circle(img, (c[0], c[1]), 2, (0, 0, 255), 3)

      # Mostrar resultado
      cv2.imshow(f"Resultado - {img_path}", img)
      cv2.waitKey(0)

  cv2.destroyAllWindows()
    </pre></code>
  </div>

  <h3>Parte 3D. Hough ao vivo com webcams</h3>
  <p>
    Na sequ√™ncia, o c√≥digo anterior foi adaptado para entrada de v√≠deo ao vivo com as duas webcams da c√¢mera est√©reo. O sistema aplicava em tempo real a Transformada de Hough para identificar formas geom√©tricas, como linhas e c√≠rculos, sobrepondo essas detec√ß√µes nos quadros capturados.
  </p>
  <p>
    A imagem abaixo foi obtida durante a execu√ß√£o do sistema, mostrando a identifica√ß√£o simult√¢nea de c√≠rculos e linhas nas imagens capturadas pelas c√¢meras:
  </p>

  <div class="image-row">
    <div class="image-col">
      <img src="relatorio5/HoughCameras.png" alt="Detec√ß√£o de linhas e c√≠rculos em v√≠deo ao vivo" class="relatorio-image">
      <p class="image-caption"><em>Exemplo de detec√ß√£o de linhas e c√≠rculos ao vivo com c√¢meras est√©reo (HoughCameras.png)</em></p>
    </div>
  </div>

  <div class="button-group">
    <button onclick="toggleSection('code-parte3d')">Ver C√≥digo - Parte 3D</button>
  </div>
  <div id="code-parte3d" class="relatorio-section" style="display: none;">
    <code><pre>
  import cv2
  import numpy as np

  cams = []
  for i in range(10):
      camera = cv2.VideoCapture(i)
      if camera.isOpened():
          cams.append(i)
      camera.release()

  # Inicializar duas webcams (ajuste os √≠ndices conforme necess√°rio)
  cam_esq = cv2.VideoCapture(cams[0])
  cam_dir = cv2.VideoCapture(cams[1])

  while True:
      ret1, frame1 = cam_esq.read()
      ret2, frame2 = cam_dir.read()

      if not ret1 or not ret2:
          print("Erro ao capturar v√≠deo")
          break

      def processar(frame):
          gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
          blurred = cv2.medianBlur(gray, 5)
          edges = cv2.Canny(blurred, 50, 150)

          # Linhas
          linhas = cv2.HoughLinesP(edges, 1, np.pi / 180, 100, 50, 10)
          if linhas is not None:
              for linha in linhas:
                  x1, y1, x2, y2 = linha[0]
                  cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

          # C√≠rculos
          circulos = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT, 1, 40,
                                      param1=50, param2=40, minRadius=70, maxRadius=150)
          if circulos is not None:
              circulos = np.uint16(np.around(circulos))
              for c in circulos[0, :]:
                  cv2.circle(frame, (c[0], c[1]), c[2], (255, 0, 0), 2)
                  cv2.circle(frame, (c[0], c[1]), 2, (0, 0, 255), 3)

          return frame

      # Processar e mostrar
      frame1_proc = processar(frame1)
      frame2_proc = processar(frame2)

      cv2.imshow("Camera Esquerda", frame1_proc)
      cv2.imshow("Camera Direita", frame2_proc)

      if cv2.waitKey(1) & 0xFF == ord('q'):
          break

  cam_esq.release()
  cam_dir.release()
  cv2.destroyAllWindows()
    </pre></code>
  </div>

  <h2>An√°lise e Discuss√£o</h2>
  <p>
    As t√©cnicas estudadas neste laborat√≥rio demonstraram grande potencial para aplica√ß√µes pr√°ticas que envolvem an√°lise de imagens em tempo real. A detec√ß√£o de caracter√≠sticas com SIFT mostrou-se robusta a varia√ß√µes de escala e rota√ß√£o, o que √© essencial para lidar com diferentes posi√ß√µes das m√£os durante a execu√ß√£o dos sinais em Libras. J√° a Transformada de Hough foi eficaz na detec√ß√£o de formas geom√©tricas simples, como linhas e c√≠rculos, sendo uma ferramenta √∫til para auxiliar na identifica√ß√£o da posi√ß√£o e orienta√ß√£o dos dedos ou das palmas das m√£os em certos sinais.
  </p>
  <p>
    No contexto do nosso projeto T1, cujo objetivo √© desenvolver um sistema de tradu√ß√£o autom√°tica de Libras para texto escrito com foco em ambientes cl√≠nicos e hospitalares, essas t√©cnicas ser√£o fundamentais. A identifica√ß√£o precisa de gestos e posi√ß√µes das m√£os em v√≠deos ao vivo requer algoritmos capazes de localizar e descrever regi√µes de interesse de forma confi√°vel e eficiente. Al√©m disso, a correspond√™ncia de features entre frames sucessivos pode ajudar no rastreamento cont√≠nuo das m√£os durante a execu√ß√£o dos sinais.
  </p>
  <p>
    Ao integrar essas t√©cnicas com modelos de classifica√ß√£o e redes neurais treinadas para reconhecer sinais espec√≠ficos da Libras, especialmente aqueles relacionados √† triagem e comunica√ß√£o cl√≠nica (como "dor", "febre", "sim", "n√£o", etc.), ser√° poss√≠vel construir uma aplica√ß√£o capaz de funcionar com uma simples webcam, oferecendo uma solu√ß√£o acess√≠vel e de grande impacto social. A pr√°tica com detec√ß√£o de caracter√≠sticas e formas durante o laborat√≥rio refor√ßa a base t√©cnica necess√°ria para alcan√ßar esse objetivo.
  </p>

  <h2>Conclus√µes</h2>
  <p>
    O laborat√≥rio permitiu compreender e aplicar t√©cnicas cl√°ssicas de vis√£o computacional voltadas √† detec√ß√£o de caracter√≠sticas e formas, como SIFT e a Transformada de Hough. A pr√°tica com v√≠deos ao vivo refor√ßou o entendimento sobre os desafios do ambiente real, como varia√ß√µes de ilumina√ß√£o, presen√ßa de ru√≠do e movimenta√ß√£o constante.
  </p>
  <p>
    Esses conhecimentos ser√£o fundamentais para o desenvolvimento do projeto T1, que envolve o reconhecimento de sinais da Libras a partir de v√≠deo em tempo real. A experi√™ncia adquirida com a extra√ß√£o e correspond√™ncia de features contribuir√° diretamente para a detec√ß√£o e rastreamento de gestos manuais, etapa essencial para o funcionamento eficaz do sistema de tradu√ß√£o em ambientes cl√≠nicos.
  </p>

  <h2>Refer√™ncias</h2>
  <ul>
    <li><a href="https://docs.opencv.org/4.x/df/d54/tutorial_py_features_meaning.html" target="_blank">Entendendo sobre Features</a>: Introdu√ß√£o aos conceitos de caracter√≠sticas em imagens e seu papel em tarefas como reconhecimento e rastreamento.</li>
    <li><a href="https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html" target="_blank">Detector de Harris</a>: T√©cnica cl√°ssica para detec√ß√£o de cantos, baseada em mudan√ßas de intensidade em diferentes dire√ß√µes.</li>
    <li><a href="https://docs.opencv.org/4.x/d4/d8c/tutorial_py_shi_tomasi.html" target="_blank">Detector de Shi-Tomasi</a>: Varia√ß√£o do detector de Harris que melhora a sele√ß√£o de cantos mais est√°veis.</li>
    <li><a href="https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html" target="_blank">Introdu√ß√£o ao SIFT</a>: Algoritmo robusto para detec√ß√£o e descri√ß√£o de pontos-chave invariante √† escala e rota√ß√£o.</li>
    <li><a href="https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html" target="_blank">Feature Matching + Homography</a>: Tutorial utilizado como base para detec√ß√£o de correspond√™ncias com SIFT e estima√ß√£o de homografia entre imagens.</li>
    <li><a href="https://learnopencv.com/hough-transform-with-opencv-c-python/" target="_blank">Transformada de Hough</a>: T√©cnica usada para detec√ß√£o de formas geom√©tricas, como linhas e c√≠rculos, em imagens com ru√≠do ou objetos parciais.</li>
  </ul>

</div>
