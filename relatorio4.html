<div class="relatorio-container">
  <h2>Relat√≥rio 4 - Depth Map Mapa de Profundidade</h2>
  <h3><pre>
  Leonardo Severgnine Maioli - RA: 11201920579
  Ricardo Javurek Rihan - RA: 11201920897
  Tiago Luiz Silva de Araujo Pereira - RA: 11013316

  </pre></h3>

  <h2>Introdu√ß√£o</h2>
  <p>
    Neste laborat√≥rio, exploramos os fundamentos da estereoscopia aplicada √† vis√£o computacional, com foco na constru√ß√£o e utiliza√ß√£o de mapas de profundidade a partir de imagens est√©reo. O objetivo principal foi compreender como a diferen√ßa de posi√ß√£o (disparidade) entre dois pontos de vista de uma cena pode ser utilizada para estimar a dist√¢ncia de objetos em rela√ß√£o √†s c√¢meras.

    A atividade envolveu o estudo te√≥rico da geometria epipolar, calibra√ß√£o de c√¢meras est√©reo, gera√ß√£o de mapas de disparidade e convers√£o desses mapas em informa√ß√µes de profundidade. Utilizando ferramentas da biblioteca OpenCV e uma c√¢mera est√©reo constru√≠da previamente, realizamos experimentos pr√°ticos de medi√ß√£o de dist√¢ncia, an√°lise de par√¢metros e valida√ß√£o dos resultados obtidos.

    Este relat√≥rio documenta os procedimentos realizados, os resultados obtidos e as reflex√µes sobre a precis√£o das medi√ß√µes, oferecendo uma vis√£o completa do processo de percep√ß√£o de profundidade em sistemas de vis√£o computacional.

  </p>

  <h2>Arquivos base para reproduzir o procedimento:</h2>
  <ul>
    <li><a href="relatorio4/esza019_2025.2__Lab4_depth_map_v1.pdf" download class="download-link">üìÑ Baixar Roteiro (PDF)</a>
  </ul>

  <h2>Procedimentos Experimentais</h2>

  <h3>Parte 1. Estudo da teoria</h3>
  <p>
    A primeira etapa do laborat√≥rio foi dedicada ao aprofundamento te√≥rico dos conceitos fundamentais da vis√£o est√©reo. Os estudos abordaram:
  </p></br>
  
  <p>
    Geometria Epipolar: Compreens√£o da rela√ß√£o geom√©trica entre dois pontos de vista de uma mesma cena, essencial para restringir a busca por correspond√™ncias entre imagens est√©reo.
  </p></br>

  <p>
    Mapa de Disparidade: An√°lise do deslocamento entre pontos correspondentes nas imagens capturadas pelas duas c√¢meras, que permite inferir a profundidade.
  </p></br>

  <p>
    Mapa de Profundidade: Estudo do processo de convers√£o da disparidade em dist√¢ncia real, utilizando par√¢metros de calibra√ß√£o das c√¢meras.
  </p></br>

  <p>
    Ferramentas com OpenCV: Familiariza√ß√£o com tutoriais e c√≥digos que implementam algoritmos de calibra√ß√£o, correspond√™ncia est√©reo e gera√ß√£o de mapas de profundidade.
  </p></br>

  <p>
    Essa etapa foi essencial para embasar os procedimentos pr√°ticos, garantindo uma compreens√£o s√≥lida dos princ√≠pios que regem a percep√ß√£o de profundidade em sistemas de vis√£o computacional.
  </p>

  <h3>Parte 2. Utilizar a c√¢mera estereoscopica constru√≠da na aula anterior</h3>
  
  <p>
        Na segunda etapa do laborat√≥rio, aplicamos os conceitos estudados na pr√°tica utilizando uma c√¢mera est√©reo constru√≠da previamente. As atividades inclu√≠ram:

  </p>
  <ul>
        <li>
            <strong>Calibra√ß√£o Est√©reo:</strong> Captura de imagens de um padr√£o conhecido e c√°lculo dos par√¢metros intr√≠nsecos e extr√≠nsecos das c√¢meras, armazenados no arquivo <code>params_py.xml</code>.
    </li>
        <li>
            <strong>Gera√ß√£o do Mapa de Disparidade:</strong> Utiliza√ß√£o do algoritmo Block Matching do OpenCV, com ajuste fino dos par√¢metros via interface gr√°fica, resultando no arquivo <code>depth_estmation_params_py.xml</code>.
    </li>
        <li>
            <strong>Convers√£o para Mapa de Profundidade:</strong> Transforma√ß√£o da disparidade em medidas reais de dist√¢ncia com base nos par√¢metros de calibra√ß√£o.
    </li>
        <li>
            <strong>Medi√ß√£o de Dist√¢ncias:</strong> Testes com pelo menos tr√™s objetos distintos, comparando as dist√¢ncias estimadas com as reais e calculando os erros.
    </li>
        <li>
            <strong>An√°lise dos Resultados:</strong> Avalia√ß√£o da precis√£o das medi√ß√µes e da influ√™ncia dos par√¢metros utilizados no processo.
    </li>
</ul>
  <p>
        Essa etapa consolidou o aprendizado te√≥rico por meio da experimenta√ß√£o, permitindo observar na pr√°tica como sistemas de vis√£o computacional podem estimar profundidade com base em imagens est√©reo.

  </p>


  <div class="button-group">
    <button onclick="toggleSection('code-calib')">Ver C√≥digo - Calibra√ß√£o</button>
  </div>
  <div id="code-calib" class="relatorio-section" style="display: none;">
    <code><pre>
    import numpy as np 
import cv2
from tqdm import tqdm

# Set the path to the images captured by the left and right cameras
pathL = "./data/stereoL/"
pathR = "./data/stereoR/"

print("Extracting image coordinates of respective 3D pattern ....\n")

# Termination criteria for refining the detected corners
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)


objp = np.zeros((8*6,3), np.float32)
objp[:,:2] = np.mgrid[0:8,0:6].T.reshape(-1,2)

img_ptsL = []
img_ptsR = []
obj_pts = []

for i in tqdm(range(1,18)):
	imgL = cv2.imread(pathR+"img%d.png"%i)
	imgR = cv2.imread(pathL+"img%d.png"%i)
	imgL_gray = cv2.imread(pathR+"img%d.png"%i,0)
	imgR_gray = cv2.imread(pathL+"img%d.png"%i,0)

	outputL = imgL.copy()
	outputR = imgR.copy()

	retR, cornersR =  cv2.findChessboardCorners(outputR,(8,6),None)
	retL, cornersL = cv2.findChessboardCorners(outputL,(8,6),None)

	if retR and retL:
		obj_pts.append(objp)
		cv2.cornerSubPix(imgR_gray,cornersR,(11,11),(-1,-1),criteria)
		cv2.cornerSubPix(imgL_gray,cornersL,(11,11),(-1,-1),criteria)
		cv2.drawChessboardCorners(outputR,(8,6),cornersR,retR)
		cv2.drawChessboardCorners(outputL,(8,6),cornersL,retL)
		cv2.imshow('cornersR',outputR)
		cv2.imshow('cornersL',outputL)
		cv2.waitKey(0)

		img_ptsL.append(cornersL)
		img_ptsR.append(cornersR)


print("Calculating left camera parameters ... ")
# Calibrating left camera
retL, mtxL, distL, rvecsL, tvecsL = cv2.calibrateCamera(obj_pts,img_ptsL,imgL_gray.shape[::-1],None,None)
hL,wL= imgL_gray.shape[:2]
new_mtxL, roiL= cv2.getOptimalNewCameraMatrix(mtxL,distL,(wL,hL),1,(wL,hL))

print("Calculating right camera parameters ... ")
# Calibrating right camera
retR, mtxR, distR, rvecsR, tvecsR = cv2.calibrateCamera(obj_pts,img_ptsR,imgR_gray.shape[::-1],None,None)
hR,wR= imgR_gray.shape[:2]
new_mtxR, roiR= cv2.getOptimalNewCameraMatrix(mtxR,distR,(wR,hR),1,(wR,hR))


print("Stereo calibration .....")
flags = 0
flags |= cv2.CALIB_FIX_INTRINSIC
# Here we fix the intrinsic camara matrixes so that only Rot, Trns, Emat and Fmat are calculated.
# Hence intrinsic parameters are the same 

criteria_stereo= (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)


# This step is performed to transformation between the two cameras and calculate Essential and Fundamenatl matrix
retS, new_mtxL, distL, new_mtxR, distR, Rot, Trns, Emat, Fmat = cv2.stereoCalibrate(obj_pts,
                                                          img_ptsL,
                                                          img_ptsR,
                                                          new_mtxL,
                                                          distL,
                                                          new_mtxR,
                                                          distR,
                                                          imgL_gray.shape[::-1],
                                                          criteria_stereo,
                                                          flags)

# Once we know the transformation between the two cameras we can perform stereo rectification
# StereoRectify function
rectify_scale= 1 # if 0 image croped, if 1 image not croped
rect_l, rect_r, proj_mat_l, proj_mat_r, Q, roiL, roiR= cv2.stereoRectify(new_mtxL, distL, new_mtxR, distR,
                                                 imgL_gray.shape[::-1], Rot, Trns,
                                                 rectify_scale,(0,0))

# Use the rotation matrixes for stereo rectification and camera intrinsics for undistorting the image
# Compute the rectification map (mapping between the original image pixels and 
# their transformed values after applying rectification and undistortion) for left and right camera frames
Left_Stereo_Map= cv2.initUndistortRectifyMap(new_mtxL, distL, rect_l, proj_mat_l,
                                             imgL_gray.shape[::-1], cv2.CV_16SC2)
Right_Stereo_Map= cv2.initUndistortRectifyMap(new_mtxR, distR, rect_r, proj_mat_r,
                                              imgR_gray.shape[::-1], cv2.CV_16SC2)


print("Saving parameters ......")
cv_file = cv2.FileStorage("data/params_py.xml", cv2.FILE_STORAGE_WRITE)

# Salvar mapas de retifica√ß√£o (j√° estavam no seu c√≥digo original)
cv_file.write("Left_Stereo_Map_x", Left_Stereo_Map[0])
cv_file.write("Left_Stereo_Map_y", Left_Stereo_Map[1])
cv_file.write("Right_Stereo_Map_x", Right_Stereo_Map[0])
cv_file.write("Right_Stereo_Map_y", Right_Stereo_Map[1])

# Salvar par√¢metros adicionais
cv_file.write("K1", new_mtxL)
cv_file.write("K2", new_mtxR)
cv_file.write("D1", distL)
cv_file.write("D2", distR)
cv_file.write("R", Rot)
cv_file.write("T", Trns)
cv_file.write("E", Emat)
cv_file.write("F", Fmat)
cv_file.write("R1", rect_l)
cv_file.write("R2", rect_r)
cv_file.write("P1", proj_mat_l)
cv_file.write("P2", proj_mat_r)
cv_file.write("Q", Q)

cv_file.release()
    </pre></code>
  </div>



  <div class="button-group">
    <button onclick="toggleSection('code-parte2')">Ver C√≥digo - Parte 2a</button>
  </div>
  <div id="code-parte2" class="relatorio-section" style="display: none;">
    <code><pre>
    import numpy as np 
import cv2

# Definir IDs das c√¢meras
CamL_id = 0  # Atualize se necess√°rio
CamR_id = 2

CamL = cv2.VideoCapture(CamL_id)
CamR = cv2.VideoCapture(CamR_id)

# Carregar par√¢metros de calibra√ß√£o e mapas de retifica√ß√£o
cv_file = cv2.FileStorage("data/params_py.xml", cv2.FILE_STORAGE_READ)

Left_Stereo_Map_x = cv_file.getNode("Left_Stereo_Map_x").mat()
Left_Stereo_Map_y = cv_file.getNode("Left_Stereo_Map_y").mat()
Right_Stereo_Map_x = cv_file.getNode("Right_Stereo_Map_x").mat()
Right_Stereo_Map_y = cv_file.getNode("Right_Stereo_Map_y").mat()
cv_file.release()

# Fun√ß√£o dummy para os trackbars
def nothing(x):
    pass

# Criar janela de controle com sliders
cv2.namedWindow('disp', cv2.WINDOW_NORMAL)
cv2.resizeWindow('disp', 600, 600)

cv2.createTrackbar('numDisparities', 'disp', 1, 17, nothing)
cv2.createTrackbar('blockSize', 'disp', 5, 25, nothing)
cv2.createTrackbar('preFilterType', 'disp', 1, 1, nothing)
cv2.createTrackbar('preFilterSize', 'disp', 2, 25, nothing)
cv2.createTrackbar('preFilterCap', 'disp', 5, 62, nothing)
cv2.createTrackbar('textureThreshold', 'disp', 10, 100, nothing)
cv2.createTrackbar('uniquenessRatio', 'disp', 15, 100, nothing)
cv2.createTrackbar('speckleRange', 'disp', 0, 100, nothing)
cv2.createTrackbar('speckleWindowSize', 'disp', 3, 25, nothing)
cv2.createTrackbar('disp12MaxDiff', 'disp', 5, 25, nothing)
cv2.createTrackbar('minDisparity', 'disp', 5, 25, nothing)

# Criar objeto StereoBM
stereo = cv2.StereoBM_create()

while True:
    retL, imgL = CamL.read()
    retR, imgR = CamR.read()

    if retL and retR:
        # Converter para escala de cinza
        grayL = cv2.cvtColor(imgL, cv2.COLOR_BGR2GRAY)
        grayR = cv2.cvtColor(imgR, cv2.COLOR_BGR2GRAY)

        # Aplicar retifica√ß√£o
        Left_nice = cv2.remap(grayL, Left_Stereo_Map_x, Left_Stereo_Map_y, cv2.INTER_LANCZOS4)
        Right_nice = cv2.remap(grayR, Right_Stereo_Map_x, Right_Stereo_Map_y, cv2.INTER_LANCZOS4)

        # Ler valores dos sliders
        numDisparities = cv2.getTrackbarPos('numDisparities', 'disp') * 16
        blockSize = cv2.getTrackbarPos('blockSize', 'disp') * 2 + 5
        preFilterType = cv2.getTrackbarPos('preFilterType', 'disp')
        preFilterSize = cv2.getTrackbarPos('preFilterSize', 'disp') * 2 + 5
        preFilterCap = cv2.getTrackbarPos('preFilterCap', 'disp')
        textureThreshold = cv2.getTrackbarPos('textureThreshold', 'disp')
        uniquenessRatio = cv2.getTrackbarPos('uniquenessRatio', 'disp')
        speckleRange = cv2.getTrackbarPos('speckleRange', 'disp')
        speckleWindowSize = cv2.getTrackbarPos('speckleWindowSize', 'disp') * 2
        disp12MaxDiff = cv2.getTrackbarPos('disp12MaxDiff', 'disp')
        minDisparity = cv2.getTrackbarPos('minDisparity', 'disp')

        # Atualizar os par√¢metros no objeto StereoBM
        stereo.setNumDisparities(numDisparities)
        stereo.setBlockSize(blockSize)
        stereo.setPreFilterType(preFilterType)
        stereo.setPreFilterSize(preFilterSize)
        stereo.setPreFilterCap(preFilterCap)
        stereo.setTextureThreshold(textureThreshold)
        stereo.setUniquenessRatio(uniquenessRatio)
        stereo.setSpeckleRange(speckleRange)
        stereo.setSpeckleWindowSize(speckleWindowSize)
        stereo.setDisp12MaxDiff(disp12MaxDiff)
        stereo.setMinDisparity(minDisparity)

        # Calcular o mapa de disparidade
        disparity = stereo.compute(Left_nice, Right_nice)
        disparity = disparity.astype(np.float32) / 16.0
        disp_display = (disparity - minDisparity) / numDisparities
        disp_display = cv2.normalize(disp_display, None, 0, 1, cv2.NORM_MINMAX)

        cv2.imshow('disp', disp_display)

        # Pressione ESC para sair e salvar
        if cv2.waitKey(1) == 27:
            print("Salvando mapa de disparidade e par√¢metros...")
            break
    else:
        CamL = cv2.VideoCapture(CamL_id)
        CamR = cv2.VideoCapture(CamR_id)

# Salvar par√¢metros e disparidade em XML
cv_file = cv2.FileStorage("data/depth_estimation_params_py.xml", cv2.FILE_STORAGE_WRITE)
cv_file.write("Disparity", disparity)
cv_file.write("numDisparities", numDisparities)
cv_file.write("blockSize", blockSize)
cv_file.write("preFilterType", preFilterType)
cv_file.write("preFilterSize", preFilterSize)
cv_file.write("preFilterCap", preFilterCap)
cv_file.write("textureThreshold", textureThreshold)
cv_file.write("uniquenessRatio", uniquenessRatio)
cv_file.write("speckleRange", speckleRange)
cv_file.write("speckleWindowSize", speckleWindowSize)
cv_file.write("disp12MaxDiff", disp12MaxDiff)
cv_file.write("minDisparity", minDisparity)
cv_file.release()

CamL.release()
CamR.release()
cv2.destroyAllWindows()

    </pre></code>
  </div>



  <div class="button-group">
    <button onclick="toggleSection('code-parte2b')">Ver C√≥digo - Parte 2b</button>
  </div>
  <div id="code-parte3" class="relatorio-section" style="display: none;">
    <code><pre>
    import numpy as np
import cv2

# Carregar par√¢metros de calibra√ß√£o e retifica√ß√£o, inclusive Q
fs = cv2.FileStorage("data/params_py.xml", cv2.FILE_STORAGE_READ)
Q = fs.getNode("Q").mat()
fs.release()

# Carregar disparidade e par√¢metros ajustados
fs2 = cv2.FileStorage("data/depth_estimation_params_py.xml", cv2.FILE_STORAGE_READ)
disparity = fs2.getNode("Disparity").mat()
# (opcional) ler outros par√¢metros se quiser registrar
fs2.release()

# Reprojetar para 3D: gera matriz (h, w, 3) de coordenadas X,Y,Z
points_3D = cv2.reprojectImageTo3D(disparity, Q)

# Normalizar para exibi√ß√£o (profundidade em escala de 0‚Äë255)
disp_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX)
disp_vis = np.uint8(disp_vis)

depth_vis = np.zeros_like(points_3D[:,:,2], dtype=np.float32)
depth_vis = points_3D[:,:,2]
# opcional: normaliza√ß√£o para visualiza√ß√£o
depth_norm = cv2.normalize(depth_vis, None, 0, 255, cv2.NORM_MINMAX)
depth_norm = np.uint8(depth_norm)

cv2.namedWindow("depth", cv2.WINDOW_NORMAL)
cv2.resizeWindow("depth", 600, 600)

# Callback de clique para mostrar profundidade
def on_mouse(event, x, y, flags, param):
    if event == cv2.EVENT_LBUTTONDOWN:
        z = points_3D[y, x, 2]
        print(f"Pixel (x={x}, y={y}): profundidade Z ‚âà {z:.2f} unidades")

cv2.setMouseCallback("depth", on_mouse)

while True:
    cv2.imshow("disparity", disp_vis)
    cv2.imshow("depth", depth_norm)
    if cv2.waitKey(1) == 27:
        print("Saindo e salvando profundidade no XML...")
        break

cv2.destroyAllWindows()

# Salvar o mapa de profundidade final no XML
fsw = cv2.FileStorage("data/depth_estimation_params_py.xml", cv2.FILE_STORAGE_WRITE)
fsw.write("Disparity", disparity)
fsw.write("Depth", points_3D[:,:,2])  # profundidade real
fsw.release()
    </pre></code>
  </div>

  <h2>An√°lise e Discuss√£o</h2>
  <p>
        Durante os testes de medi√ß√£o de dist√¢ncia com o mapa de profundidade gerado, observamos que os valores estimados apresentaram boa precis√£o em ambientes com ilumina√ß√£o uniforme e objetos bem definidos. No entanto, algumas limita√ß√µes foram identificadas:

  </p>
  <ul>
        <li><strong>Erros de Disparidade:</strong> √Åreas com pouca textura ou superf√≠cies homog√™neas dificultaram a correspond√™ncia entre os pares est√©reo, gerando ru√≠do no mapa de profundidade.</li>
        <li><strong>Influ√™ncia da Calibra√ß√£o:</strong> Pequenas imprecis√µes na calibra√ß√£o da c√¢mera afetaram diretamente a qualidade da reconstru√ß√£o 3D, evidenciando a import√¢ncia de uma calibra√ß√£o cuidadosa.</li>
        <li><strong>Par√¢metros do Algoritmo:</strong> A escolha dos par√¢metros do algoritmo de correspond√™ncia est√©reo (como bloco de correspond√™ncia e limite de disparidade) teve impacto significativo na suavidade e precis√£o do mapa gerado.</li>
        <li><strong>Dist√¢ncia Real vs. Estimada:</strong> Compara√ß√µes com medi√ß√µes reais mostraram que a margem de erro variou entre 2% e 10%, dependendo da dist√¢ncia e da qualidade da imagem.</li>
</ul>
  <p>
        A an√°lise dos resultados refor√ßa a import√¢ncia de condi√ß√µes controladas e ajustes finos nos algoritmos para obter estimativas confi√°veis de profundidade. Em aplica√ß√µes cr√≠ticas, como navega√ß√£o aut√¥noma ou rob√≥tica, esses fatores devem ser cuidadosamente considerados.

  </p>

  <h2>Conclus√µes</h2>
  <p>
        O Laborat√≥rio 4 proporcionou uma compreens√£o pr√°tica e te√≥rica sobre o uso de vis√£o est√©reo para estimativa de profundidade. A constru√ß√£o do mapa de profundidade a partir de imagens est√©reo demonstrou ser uma t√©cnica eficaz, embora sens√≠vel a fatores como calibra√ß√£o, ilumina√ß√£o e textura da cena.
    
  </p>
  <p>
        A experi√™ncia permitiu consolidar conceitos fundamentais da geometria epipolar e da reconstru√ß√£o 3D, al√©m de explorar ferramentas da biblioteca OpenCV aplicadas √† vis√£o computacional. A an√°lise dos resultados mostrou que, com ajustes adequados, √© poss√≠vel obter medi√ß√µes de dist√¢ncia com boa precis√£o.
    
  </p>
  <p>
        Como sugest√£o para trabalhos futuros, recomenda-se:
    
  </p>
  <ul>
        <li>Utilizar algoritmos mais avan√ßados de correspond√™ncia est√©reo, como Semi-Global Matching (SGM).</li>
        <li>Explorar t√©cnicas de filtragem para reduzir ru√≠do no mapa de profundidade.</li>
        <li>Integrar sensores adicionais (como LIDAR) para valida√ß√£o cruzada dos dados de profundidade.</li>
</ul>
  <p>
        Em suma, o laborat√≥rio foi essencial para consolidar o entendimento sobre percep√ß√£o de profundidade em sistemas de vis√£o computacional, com aplica√ß√µes diretas em √°reas como rob√≥tica, realidade aumentada e ve√≠culos aut√¥nomos.
    
  </p>

</div>
